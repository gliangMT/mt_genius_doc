

<!DOCTYPE html>


<html lang="zh-CN" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>vLLM_MTTransformer &#8212; Moore Threads AI Doc v0.1 文档</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../_static/translations.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'vllm/vllm_mtt';</script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="prev" title="&lt;no title&gt;" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="zh-CN"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Moore Threads AI Doc v0.1 文档</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">vLLM_MTTransformer</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="下载此页面">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/vllm/vllm_mtt.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="下载源文件"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="列印成 PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="全屏模式"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="搜索" aria-label="搜索" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>vLLM_MTTransformer</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> 目录 </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">0.环境依赖</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.启动容器</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2.支持模型列表</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm">3.vLLM参数配置</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4.快速开始</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark">5.Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">前置准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">运行测试</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mtt">6.mtt性能测试</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">新建配置文件-示例</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perf">执行perf测试</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">查验结果</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">7.附录</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Benchmark离线结果解释</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Benchmark在线结果解释</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="vllm-mttransformer">
<h1>vLLM_MTTransformer<a class="headerlink" href="#vllm-mttransformer" title="此标题的永久链接">#</a></h1>
<p>MTTransformer为摩尔线程自研的LLM推理引擎，vLLM是一个快速且易于使用的LLM推理和服务库，本文档将介绍vLLM的MTTransformer适配版本使用方法。</p>
<section id="id1">
<h2>0.环境依赖<a class="headerlink" href="#id1" title="此标题的永久链接">#</a></h2>
<ul class="simple">
<li><p>MT GPU Driver：<a class="reference external" href="https://oss.mthreads.com:9001/buckets/product-release/browse/cmVsZWFzZS1rdWFlLTEuMi4wL0V4dGVybmFsL2Rka194ODYtbXRncHVfbGludXgteG9yZy1yZWxlYXNlLXBkdW1wX29mZi5kZWI=">kuae 1.2</a></p></li>
<li><p>mtbios：<a class="reference external" href="https://confluence.mthreads.com/pages/viewpage.action?spaceKey=MTBIOS&amp;title=7.4.3.3+QY2+MT-BIOS+V3.4.3">QY2 v3.4.3</a></p></li>
<li><p>code: <a class="reference external" href="https://sh-code.mthreads.com/ai/vllm/-/tree/vllm_mtt?ref_type=heads">Vllm</a> (下文中容器中包含代码)</p></li>
</ul>
</section>
<section id="id2">
<h2>1.启动容器<a class="headerlink" href="#id2" title="此标题的永久链接">#</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 工区</span>
<span class="n">docker</span> <span class="n">run</span> <span class="o">-</span><span class="n">it</span> <span class="o">--</span><span class="n">privileged</span> <span class="o">--</span><span class="n">net</span> <span class="n">host</span> <span class="o">--</span><span class="n">name</span><span class="o">=</span><span class="n">vllm_mtt_test</span> <span class="o">-</span><span class="n">w</span> <span class="o">/</span><span class="n">workspace</span> <span class="o">-</span><span class="n">v</span> <span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">mtt</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">mtt</span><span class="o">/</span> <span class="o">--</span><span class="n">env</span> <span class="n">MTHREADS_VISIBLE_DEVICES</span><span class="o">=</span><span class="nb">all</span> <span class="o">--</span><span class="n">shm</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">80</span><span class="n">g</span> <span class="n">sh</span><span class="o">-</span><span class="n">harbor</span><span class="o">.</span><span class="n">mthreads</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">mt</span><span class="o">-</span><span class="n">ai</span><span class="o">/</span><span class="n">musa</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">transformer</span><span class="o">-</span><span class="n">vllm</span><span class="p">:</span><span class="n">v0</span><span class="mf">.1.4</span> <span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">bash</span>

<span class="c1"># 中联集群</span>
<span class="n">docker</span> <span class="n">run</span> <span class="o">-</span><span class="n">it</span> <span class="o">--</span><span class="n">privileged</span> <span class="o">--</span><span class="n">net</span> <span class="n">host</span> <span class="o">--</span><span class="n">name</span><span class="o">=</span><span class="n">vllm_mtt_test</span> <span class="o">-</span><span class="n">w</span> <span class="o">/</span><span class="n">workspace</span> <span class="o">-</span><span class="n">v</span> <span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">mtt</span><span class="o">/</span><span class="p">:</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">mtt</span><span class="o">/</span> <span class="o">--</span><span class="n">env</span> <span class="n">MTHREADS_VISIBLE_DEVICES</span><span class="o">=</span><span class="nb">all</span> <span class="o">--</span><span class="n">shm</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">80</span><span class="n">g</span> <span class="n">core</span><span class="o">.</span><span class="n">harbor</span><span class="o">.</span><span class="n">zlidc</span><span class="o">.</span><span class="n">mthreads</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">30003</span><span class="o">/</span><span class="n">mt</span><span class="o">-</span><span class="n">ai</span><span class="o">/</span><span class="n">musa</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">transformer</span><span class="o">-</span><span class="n">vllm</span><span class="p">:</span><span class="n">v0</span><span class="mf">.1.4</span> <span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">bash</span>
</pre></div>
</div>
<p>关键参数解释</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--name=vllm_mtt_test</span></code> 指定容器名称，不能和其他容器重名</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-w</span> <span class="pre">/workspace</span></code> 指定工作目录</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-v</span> <span class="pre">/data/mtt:/data/mtt/</span></code>   映射目录，前者为物理机目录，后者为容器内访问的目录，该目录可访问模型文件</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sh-harbor.mthreads.com/mt-ai/musa-pytorch-transformer-vllm:v0.1.4</span></code>是 <code class="docutils literal notranslate"><span class="pre">镜像名称：tag</span></code></p></li>
</ul>
</section>
<section id="id3">
<h2>2.支持模型列表<a class="headerlink" href="#id3" title="此标题的永久链接">#</a></h2>
<table border="1" class="docutils">
<thead>
<tr>
<th>模型名称</th>
<th>验证过的模型</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama2</td>
<td>7B, 13B, 70B</td>
<td>全系列模型都支持</td>
</tr>
<tr>
<td>Mistral</td>
<td>7B</td>
<td></td>
</tr>
<tr>
<td>Qwen</td>
<td>7B, 14B, 72B</td>
<td>全系列模型都支持</td>
</tr>
<tr>
<td>Qwen1.5</td>
<td>32B, 72B, 110B</td>
<td>Qwen1.5和2同构，非MOE模型都支持</td>
</tr>
<tr>
<td>Qwen2</td>
<td>1.5B</td>
<td></td>
</tr>
<tr>
<td>ChatGLM2/3</td>
<td>6B</td>
<td></td>
</tr>
<tr>
<td>Baichuan</td>
<td>7B,13B</td>
<td></td>
</tr>
<tr>
<td>Yayi2</td>
<td>30B</td>
<td>由于原生的vllm不支持yayi，需要增加环境变量，执行 <code>export VLLM_NO_USAGE_STATS=1</code> 绕开某些vllm的bug</td>
</tr>
</tbody>
</table></section>
<section id="vllm">
<h2>3.vLLM参数配置<a class="headerlink" href="#vllm" title="此标题的永久链接">#</a></h2>
<p>关于vLLM部分的一些参数 配置可以参考官网相关文档<a class="reference external" href="https://docs.vllm.ai/en/latest/models/engine_args.html">Engine Arguments</a>和<a class="reference external" href="https://docs.vllm.ai/en/latest/dev/sampling_params.html">Sampling Parameters</a>，这里同时罗列一些vLLM_MTTransformer不支持的参数：</p>
<ol class="simple">
<li><p>由于MTTansformer仅支持float16模式，所以目前不支持量化相关的参数</p></li>
<li><p>由于多卡环境是在MTTransformer内部处理的，所以目前ray相关的参数（分布式相关资源调度）不支持</p></li>
<li><p>rope相关参数（位置编码相关，用于优化计算和保持对输入序列中各个词的位置信息的理解），暂不支持</p></li>
<li><p>lora(一种fine tune技术)暂不支持</p></li>
</ol>
<p>目前支持的参数：</p>
<ul class="simple">
<li><p>model：需要指定经过 <code class="docutils literal notranslate"><span class="pre">mttransformer.convert_weigh</span></code>转换之后的权重目录。需要注意的如果是之前mtt版本转换过的权重，则不兼容当前版本，需要重新转换</p></li>
<li><p>device：仅支持设置为musa</p></li>
<li><p>tensor-parallel-size： 必须和convert_weight的模型一致，如果是多卡推理，则该参数必须设置（vllm中申请kv cache需要这个信息）</p></li>
<li><p>dtype: 目前仅支持默认值auto或者float16</p></li>
<li><p>kv-cache-dtype：仅支持默认值auto</p></li>
<li><p>pipeline-parallel-size：仅支持默认值1</p></li>
<li><p>block-size：仅支持设置为64</p></li>
<li><p>max-num-seqs：建议设置为128以获取最佳性能</p></li>
<li><p>max_num_batched_tokens，max_model_len ：需要根据运行的序列长度进行配置</p></li>
</ul>
</section>
<section id="id4">
<h2>4.快速开始<a class="headerlink" href="#id4" title="此标题的永久链接">#</a></h2>
<p>以llama2-7B模型为示例：</p>
<ol>
<li><p>权重转换</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>mttransformer.convert_weight<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--in_file<span class="w"> </span>/data/mtt/models/llama-2-7b-chat-hf-fp16/<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--saved_dir<span class="w"> </span>/data/mtt/models_convert/llama-2-7b-chat-hf-fp16-tp1-convert/<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--weight_data_type<span class="w"> </span>fp16<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_type<span class="w"> </span>mistral<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--tensor-para-size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--arch<span class="w"> </span>mp_22
</pre></div>
</div>
<p>其中：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">in_file</span></code>是从huggingface上下载的模型pytorch格式权重文件</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">saved_dir</span></code>为MTTransformer转换之后的权重文件路径</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_data_type</span></code>目前仅支持fp16。值得一提的是如果in_file的模型是fp32, fp16, bf16，都会被mtt转为fp16</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_type</span></code>目前支持llama, mistral, chatglm2, baichuan, qwen, qwen2, yayi；mistral和llama同构，model_type也可以填llama；chatglm2和chatglm3同构，model_type都是chatglm2；Qwen1.5和Qwen2同构，model_type都是qwen2，但是Qwen的依然是qwen；baichuan, baichuan2，model_type都是baichuan</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">arch</span></code>仅支持mp_22，也就是qy2</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--tensor-para-size</span></code>,<code class="docutils literal notranslate"><span class="pre">-tp</span></code>张量并行数，可简单理解为参与运行的GPU数</p></li>
</ul>
<p>为方便可以执行 <code class="docutils literal notranslate"><span class="pre">convert_weight.sh</span></code>脚本进行转换：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Usage: ./convert_weight.sh &lt;original_model_path&gt; &lt;tp_size&gt;</span>
./convert_weight.sh<span class="w"> </span>/data/mtt/models/llama-2-7b-chat-hf-fp16/<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</li>
<li><p>设置环境变量</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 以上文中启动的容器为例，将vllm_mtt代码路径设置为python环境变量</span>
<span class="n">export</span> <span class="n">PYTHONPATH</span><span class="o">=/</span><span class="n">home</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">vllm_mtt</span>
</pre></div>
</div>
</li>
<li><p>运行普通示例程序</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># generate_llama2-7b.py

import vllm
from vllm import LLM, SamplingParams

# path为转换后的权重文件目录，可根据实际更改
path = &quot;/data/mtt/models_convert/llama-2-7b-chat-hf-fp16-tp1-convert/&quot; 
llm = LLM(model=path, gpu_memory_utilization = 0.9, tensor_parallel_size = 1, device = &quot;musa&quot;, block_size = 64, max_num_seqs = 128, max_model_len = 2048, max_num_batched_tokens=2048)
# Print the outputs.
prompts = [
    &quot;[INST] Hello! [/INST]&quot;,
    &quot;[INST] Hi! who are you? [/INST]&quot;,
    &quot;[INST] I am going to Paris, where should I go? [/INST]&quot;,
    &quot;[INST] What is your name? [/INST]&quot;,
    &quot;[INST] Where are you coming from? [/INST]&quot;,
  ]
sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=40)
outputs = llm.generate(prompts, sampling_params)
print(f&quot;\n&quot;)
for idx, output in enumerate(outputs):
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f&quot;Prompt {idx}: {prompt}, \nGenerated text: {generated_text}\n\n&quot;)
`python generate_llama2-7b.py` 执行示例程序即可
</pre></div>
</div>
<p>也可执行 <code class="docutils literal notranslate"><span class="pre">generate_chat.py</span></code>脚本</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># usage: generate_chat.py [-h] -ckpt CHECKPOINT_PATH -tp TENSOR_PARALLEL_SIZE</span>
python<span class="w"> </span>generate.py<span class="w"> </span>-ckpt<span class="w"> </span>/data/mtt/models_convert/llama-2-7b-chat-hf-fp16-tp2-convert/<span class="w"> </span>-tp<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
</li>
<li><p>运行流式输出示例程序</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># async_chat_llama2-7b.py</span>

<span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">AsyncLLMEngine</span><span class="p">,</span> <span class="n">AsyncEngineArgs</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;/data/mtt/models_convert/llama-2-7b-chat-hf-fp16-convert&quot;</span>
<span class="n">engine_args</span> <span class="o">=</span> <span class="n">AsyncEngineArgs</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
                        <span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
                        <span class="n">enforce_eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
			<span class="n">disable_log_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  
			<span class="n">enable_prefix_caching</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;musa&quot;</span><span class="p">,</span> 
			<span class="n">block_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> 
			<span class="n">max_num_seqs</span> <span class="o">=</span> <span class="mi">128</span><span class="p">)</span>
<span class="c1"># initialize engine and request arguments</span>

<span class="c1"># engine_args = AsyncEngineArgs(model=&quot;facebook/opt-125m&quot;, enforce_eager=True)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AsyncLLMEngine</span><span class="o">.</span><span class="n">from_engine_args</span><span class="p">(</span><span class="n">engine_args</span><span class="p">)</span>
<span class="n">sampling_param</span><span class="o">=</span><span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">build_prompt</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">generate_streaming</span><span class="p">():</span>
    <span class="n">history_text</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&gt;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prompt</span> <span class="o">==</span> <span class="s1">&#39;stop&#39;</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="n">prompt</span><span class="o">=</span><span class="n">build_prompt</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="n">history_text</span><span class="o">+=</span><span class="n">prompt</span>
        <span class="c1"># results_generator = model.generate(prompt, SamplingParams(temperature=0.0, max_tokens=1024), request_id=time.monotonic())</span>
        <span class="n">results_generator</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">history_text</span><span class="p">,</span> <span class="n">sampling_param</span><span class="p">,</span> <span class="n">request_id</span><span class="o">=</span><span class="n">time</span><span class="o">.</span><span class="n">monotonic</span><span class="p">())</span>
        <span class="n">previous_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Prompt:</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="se">\n</span><span class="s1">GenerateText:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">request_output</span> <span class="ow">in</span> <span class="n">results_generator</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">request_output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">previous_text</span><span class="p">):],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">previous_text</span> <span class="o">=</span> <span class="n">text</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># optional</span>
        <span class="c1"># previous_text = previous_text + model.engine.get_tokenizer().eos_token + &quot;\n&quot;</span>
        <span class="n">history_text</span> <span class="o">+=</span> <span class="n">previous_text</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;!!!!!!!!!!!!!!!!!!!finish a dialog!!!!!!!!!!!!!!!!!!!!!!!</span><span class="se">\n</span><span class="si">{</span><span class="n">history_text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># break</span>



<span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">generate_streaming</span><span class="p">())</span>
</pre></div>
</div>
<p>运行 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">async_chat_llama2-7b.py</span></code> 即可</p>
<p>为方便可以执行 <code class="docutils literal notranslate"><span class="pre">streaming_chat.py</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># usage: streaming_chat.py [-h] -ckpt CHECKPOINT_PATH -tp TENSOR_PARALLEL_SIZE</span>
<span class="n">python</span> <span class="n">streaming_chat</span> <span class="o">-</span><span class="n">ckpt</span> <span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">mtt</span><span class="o">/</span><span class="n">models_convert</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">chat</span><span class="o">-</span><span class="n">hf</span><span class="o">-</span><span class="n">fp16</span><span class="o">-</span><span class="n">convert</span> <span class="o">-</span><span class="n">tp</span> <span class="mi">1</span>
</pre></div>
</div>
</li>
<li><p>启动openai API Server服务</p>
<blockquote>
<div><p>vLLM 支持通过提供与 OpenAI 类似的 API 接口，使用户能够无缝地切换或替代 OpenAI 的 API，使得开发者可以使用与调用 OpenAI API 几乎相同的方式，来调用部署在 vLLM 上的模型，简化了大模型推理的部署和使用。
若需要对外开放端口，请联系IT部门同事</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.openai.api_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>/data/mtt/models_convert/chatglm3-6b-fp16-tp1-convert<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tensor-parallel-size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-pp<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--block-size<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-model-len<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--disable-log-stats<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--disable-log-requests<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.95<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span><span class="s2">&quot;musa&quot;</span>
</pre></div>
</div>
<p>测试服务</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 可以在启动服务的同一容器下启动新终端进行测试</span>

<span class="c1"># 通过api调用模型</span>
curl<span class="w"> </span>http://0.0.0.0:8000/v1/chat/completions<span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">        &quot;model&quot;: &quot;/data/mtt/models_convert/chatglm3-6b-fp16-tp1-convert&quot;,</span>
<span class="s1">        &quot;messages&quot;: [</span>
<span class="s1">            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
<span class="s1">            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;}</span>
<span class="s1">        ]</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p>期望输出</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="s2">&quot;cmpl-8062e4a30b1f452885f65d60a0d54591&quot;</span><span class="p">,</span><span class="nt">&quot;object&quot;</span><span class="p">:</span><span class="s2">&quot;chat.completion&quot;</span><span class="p">,</span><span class="nt">&quot;created&quot;</span><span class="p">:</span><span class="mi">1724259639</span><span class="p">,</span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="s2">&quot;/data/mtt/models_convert/chatglm3-6b-fp16-tp1-convert&quot;</span><span class="p">,</span><span class="nt">&quot;choices&quot;</span><span class="p">:[{</span><span class="nt">&quot;index&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="nt">&quot;message&quot;</span><span class="p">:{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot; \n The Los Angeles Dodgers won the 2020 World Series.&quot;</span><span class="p">},</span><span class="nt">&quot;logprobs&quot;</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="nt">&quot;finish_reason&quot;</span><span class="p">:</span><span class="s2">&quot;stop&quot;</span><span class="p">,</span><span class="nt">&quot;stop_reason&quot;</span><span class="p">:</span><span class="kc">null</span><span class="p">}],</span><span class="nt">&quot;usage&quot;</span><span class="p">:{</span><span class="nt">&quot;prompt_tokens&quot;</span><span class="p">:</span><span class="mi">32</span><span class="p">,</span><span class="nt">&quot;total_tokens&quot;</span><span class="p">:</span><span class="mi">49</span><span class="p">,</span><span class="nt">&quot;completion_tokens&quot;</span><span class="p">:</span><span class="mi">17</span><span class="p">}}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="benchmark">
<h2>5.Benchmark<a class="headerlink" href="#benchmark" title="此标题的永久链接">#</a></h2>
<p>这里的benchmark脚本参考vllm官方代码略作修改，具体相关的内容可以参考<a class="reference external" href="https://docs.vllm.ai/en/latest/performance_benchmark/benchmarks.html">vllm官方文档</a>。</p>
<section id="id5">
<h3>前置准备<a class="headerlink" href="#id5" title="此标题的永久链接">#</a></h3>
<p>在线测试数据集准备：</p>
<blockquote>
<div><p>ShareGPT_V3_unfiltered_cleaned_split.json数据集是用户与 GPT 模型的对话数据。这类数据集往往被用于训练和微调对话模型，以提升模型的生成能力和对话质量</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 若无法下载数据集，可以登录网站手动下载</span>
<span class="nb">cd</span><span class="w"> </span>dataset
wget<span class="w"> </span>https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json
</pre></div>
</div>
</section>
<section id="id6">
<h3>运行测试<a class="headerlink" href="#id6" title="此标题的永久链接">#</a></h3>
<p>为方便测试，将测试命令写在了脚本中，相关参数可做参考。</p>
<p>这里都以chatglm2-6B为例：</p>
<ol class="simple">
<li><p>离线测试</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>vllm-mtt_benchmark/benchmark_offline
./Chatglm2-6B.sh

<span class="c1"># 脚本中实际执行的命令</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/home/workspace/vllm_mtt
mkdir<span class="w"> </span>-p<span class="w"> </span>./bench_results
python<span class="w"> </span>benchmark_thoughput.py<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--dataset<span class="w"> </span>../dataset/train.jsonl<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--output-len<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="w"> </span>/data/mtt/models_convert/chatglm2-6b-fp16-tp1-convert/<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--max-model-len<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.9<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--save-result<span class="w"> </span>--result-dir<span class="w"> </span>bench_results/<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--num-prompts<span class="w"> </span><span class="m">7000</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--output-len<span class="w"> </span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--kv-cache-dtype<span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--device<span class="w"> </span><span class="s2">&quot;musa&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>-tp<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<ol class="simple">
<li><p>在线测试</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>vllm-mtt_benchmark/benchmark_serving/Chatglm2-6B
./step1_server_start.sh

<span class="c1"># 脚本实际执行命令</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/home/workspace/vllm_mtt
<span class="c1"># chatglm2支持设置参数 --max-model-len 4096，其他case仅为默认值2048</span>
python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.openai.api_server<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="w"> </span>/data/mtt/models_convert/chatglm2-6b-fp16-tp1-convert/<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--trust-remote-code<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--tensor-parallel-size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>-pp<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--block-size<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--max-model-len<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--disable-log-stats<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--disable-log-requests<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.95<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--device<span class="w"> </span><span class="s2">&quot;musa&quot;</span><span class="w"> </span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 可以在同一容器下另外启动一个终端</span>
./step2_test.sh

<span class="c1"># 脚本实际执行命令</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/home/workspace/vllm_mtt
mkdir<span class="w"> </span>-p<span class="w"> </span>./serving_result
python<span class="w"> </span>../benchmark_serving.py<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--dataset-path<span class="w"> </span>../../dataset/ShareGPT_V3_unfiltered_cleaned_split.json<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="w"> </span>/data/mtt/models_convert/chatglm2-6b-fp16-tp1-convert/<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--save-result<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--result-dir<span class="w"> </span>./serving_result<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<ol>
<li><p><code class="docutils literal notranslate"><span class="pre">JSON</span></code>结果转化 <code class="docutils literal notranslate"><span class="pre">CSV</span></code>
为方便查看，可以利用 <code class="docutils literal notranslate"><span class="pre">vllm-mtt_benchmark/result_convert.py</span></code>脚本进行转换，用法如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 用法</span>
<span class="c1"># usage: result_convert.py [-h] [-i IN_PATH] -m {offline,serving} [-o OUTPUT_PATH]</span>

<span class="c1"># 离线测试结果转换————示例</span>
python<span class="w"> </span>result_convert.py<span class="w"> </span>-i<span class="w"> </span>./benchmark_offline/bench_results<span class="w"> </span>-o<span class="w"> </span>./offline_result.csv<span class="w"> </span>-m<span class="w"> </span>offline

<span class="c1"># 在线测试结果转换————示例</span>
python<span class="w"> </span>result_convert.py<span class="w"> </span>-i<span class="w"> </span>./benchmark_serving/serving_result<span class="w"> </span>-o<span class="w"> </span>./serving_result.csv<span class="w"> </span>-m<span class="w"> </span>serving
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="mtt">
<h2>6.mtt性能测试<a class="headerlink" href="#mtt" title="此标题的永久链接">#</a></h2>
<section id="id7">
<h3>新建配置文件-示例<a class="headerlink" href="#id7" title="此标题的永久链接">#</a></h3>
<p>创建配置文件 <code class="docutils literal notranslate"><span class="pre">perf_config.json</span></code>，注意实际测试可以删掉注释</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>[
  {
    &quot;model_name&quot;: &quot;llama2-70b&quot;, # 以实际模型名为准
    &quot;path&quot;: &quot;/data/mtt/models_convert/llama-2-70b-hf-fp16-tp8-convert&quot;, # 对应模型路径
    &quot;batchs&quot;: [1,2,4,8], # 可自行调整
    &quot;prefill_token_lens&quot;: [256,512,1024], # 可自行调整
    &quot;decode_token_lens&quot;: [64]  # 可自行调整
  }
]
</pre></div>
</div>
</section>
<section id="perf">
<h3>执行perf测试<a class="headerlink" href="#perf" title="此标题的永久链接">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">mttransformer</span><span class="o">.</span><span class="n">perf_test</span> <span class="n">perf_config</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>查验结果<a class="headerlink" href="#id8" title="此标题的永久链接">#</a></h3>
<p>在执行路径下，测试结果保存 <code class="docutils literal notranslate"><span class="pre">./perf_data/combine_ret.csv</span></code>文件，输出如下图所示（此图不代表最终结果）:</p>
<p><img alt="img" src="../_static/mtt_perf_data.png" /></p>
</section>
</section>
<section id="id9">
<h2>7.附录<a class="headerlink" href="#id9" title="此标题的永久链接">#</a></h2>
<section id="id10">
<h3>Benchmark离线结果解释<a class="headerlink" href="#id10" title="此标题的永久链接">#</a></h3>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Successful</span> <span class="pre">requests</span></code>：读取prompts数据集总共处理成功的请求总数，代码中为num_requests，可在测试中指定大小</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">latency</span></code>：处理完所有requests所用的时间，单位为秒，在vllm源文件中此值为elapsed_time，且没有写单位（已和京东反馈，答复是不关注这个结果）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tokens</span></code>: prompt数据集中输入的给模型的token总数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">tokens</span></code>：输出tokens总数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Req/s</span></code>：每秒处理的请求数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">token</span> <span class="pre">throughput</span> <span class="pre">(tok/s)</span></code>：吞吐量，等于输出tokens总数 / 总耗时（即elapsed_time=latency）</p></li>
</ol>
</section>
<section id="id11">
<h3>Benchmark在线结果解释<a class="headerlink" href="#id11" title="此标题的永久链接">#</a></h3>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Succesful</span> <span class="pre">request</span></code>：总请求数，默认为1000</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Benchmark</span> <span class="pre">duration</span> <span class="pre">(s)</span></code>：处理完所有请求所用的时间，单位是s</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Total</span> <span class="pre">input</span> <span class="pre">tokens</span></code>: 累计的输入tokens总数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Total</span> <span class="pre">generated</span> <span class="pre">tokens</span></code>: 生成的tokens总数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Request</span> <span class="pre">throughput</span> <span class="pre">(req/s)</span></code>：每秒处理请求数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">token</span> <span class="pre">throughput</span> <span class="pre">(tok/s)</span></code>：输入token的吞吐量=Total input tokens / Benchmark duration</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">token</span> <span class="pre">throughput</span> <span class="pre">(tok/s)</span></code>:输出token吞吐量=Total generated tokens/Benchmark duration</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">TTFT</span> <span class="pre">(ms)</span></code>: 该prompt数据集经过模型处理的平均首字时延，单位ms</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Median</span> <span class="pre">TTFT</span> <span class="pre">(ms)</span></code>: 该prompt数据集经过模型处理的首字时延中位数，单位ms</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">P99</span> <span class="pre">TTFT</span> <span class="pre">(ms)</span></code>: 该prompt数据集经过模型处理的99%的请求的首字时延，单位ms（表示99%的请求的首字延迟在该时间内）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">TPOT</span> <span class="pre">(ms)</span></code>:除去首字延时，每个token的平均处理时长</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Median</span> <span class="pre">TPOT</span> <span class="pre">(ms)</span></code>: 除去首字延时，每个token处理时长的中位数</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">P99</span> <span class="pre">TPOT</span> <span class="pre">(ms)</span></code>: 除去首字延时，99%的单token处理时长在该时间内处理完成</p></li>
</ol>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">上一页</p>
        <p class="prev-next-title">&lt;no title&gt;</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 目录
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">0.环境依赖</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.启动容器</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2.支持模型列表</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm">3.vLLM参数配置</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4.快速开始</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark">5.Benchmark</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">前置准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">运行测试</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mtt">6.mtt性能测试</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">新建配置文件-示例</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perf">执行perf测试</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">查验结果</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">7.附录</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Benchmark离线结果解释</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Benchmark在线结果解释</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
作者： Moore Threads GPU Genius Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, Moore Threads.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>